================ Create output folder
================ Start training one of linear models
LINEAR_linear_w4.25 

Using pretrained decoder: False 

[Init] Using cuda.
[Init] Logging at ./experiments/log/LINEAR/LINEAR_linear_w4.25.
[Init] Initialized random seed with 3.

[Vocab] Building vocabulary.

[Data Wrangling] Read 651979 lines of code.
[Data Wrangling] 645485 valid lines / 651979 lines
[Vocab] Processing 500 lines of code...
[Vocab] Saved vocabulary at ./experiments/vocab/LINEAR_linear_w4.25
[Vocab] Vocabulary size: 924

[Data Wrangling] Read 651979 lines of code.
[Data Wrangling] 645485 valid lines / 651979 lines

[Dataset] Processing 500 source and target lines...
[Dataset] From 645485 lines, selected 500 lines (max: 20)
[Data] Created 3 batches (batch size: 128)

[Dataset] Processing 128 source and target lines...
[Dataset] From 128 lines, selected 128 lines (max: 20)
[Data] Created 1 batches (batch size: 128)

[Model] Build model.

[Train] Training for 1 epochs.
================ Start training one of constrained models
CONSTRAINED_constrained_eps0.4 

Using pretrained decoder: False 

[Init] Using cuda.
[Init] Logging at ./experiments/log/CONSTRAINED/CONSTRAINED_constrained_eps0.4.
[Init] Initialized random seed with 3.

[Vocab] Building vocabulary.

[Data Wrangling] Read 651979 lines of code.
[Data Wrangling] 645485 valid lines / 651979 lines
[Vocab] Processing 500 lines of code...
[Vocab] Saved vocabulary at ./experiments/vocab/CONSTRAINED_constrained_eps0.4
[Vocab] Vocabulary size: 924

[Data Wrangling] Read 651979 lines of code.
[Data Wrangling] 645485 valid lines / 651979 lines

[Dataset] Processing 500 source and target lines...
[Dataset] From 645485 lines, selected 500 lines (max: 20)
[Data] Created 3 batches (batch size: 128)

[Dataset] Processing 128 source and target lines...
[Dataset] From 128 lines, selected 128 lines (max: 20)
[Data] Created 1 batches (batch size: 128)

[Model] Build model.

[Train] Training for 1 epochs.
