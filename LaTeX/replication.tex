In the first experiment the autoencoder model from \citeA{autocomplete} was replicated. 

\section{Method}
To replicate the model, an encoder-decoder model was made. The encoder chooses which words to keep as keywords, and the decoder tries to retrieve the full sentence of the keywords. 

\subsection{Data}
\label{sec:data}
% TODO: make a table with a few example sentences
% TODO: descirbe train, validation and test set
The same data was used as in \citeA{autocomplete}.
The data used to train the model consisted of 500K randomly sampled sentences from the Yelp restaurant reviews corpus \cite{data}.
Another 50K sentences were used to evaluate the model. 
The sentences had at most 16 tokens. 
The reviews were segmented into sentences following the same procedure as in \citeA{GuuKelvin2018GSbE}. 

The model of \citeA{autocomplete} also predicts capital letters and whitespaces. 
However, for English we can just assume that after every word a space is used. 
Therefore, the data was adjusted to leave out the characters for whitespaces. 
Something similar is also true for capital letters: usually these only occur at the start of a sentence or in names. 
It is thus not necessary to also predict capital letters. 

\noindent\textcolor{red}{TODO: \begin{enumerate}
    \item Table with example sentences
    \item Adjust the number of sentences used for the training and the validation set (Due to computational resources \dots)
\end{enumerate}}

\subsection{Experimental Design}
The model used is an encoder-decoder model.
The encoder, using the encoding strategy $q_{\alpha}(z|x)$, takes as input a target sequence $x = (x_1, \dots, x_m)$ and outputs a sequence of keywords $z = (z_1, \dots, z_n)$.
The decoder, using the decoding strategy $p_{\beta}(x|z)$, then takes these keywords as input and outputs a predicted sequence $y = (y_1, \dots, y_k)$. 
The better the autoencoder works, the more likely it is that $x$ and $y$ are equal. 

\subsection{Model description}
\label{sec:model}
% TODO: add diagram?
\paragraph{Encoder.} 
The encoder embeds the tokens and uses a uni-directional LSTM \cite{LSTM} to score the tokens.
An additional linear layer followed by sigmoid function is used to determine the probability of keeping each token. 
From these probabilities, a mask is sampled from a Bernoulli distribution. 
Finally, the sequence of kept tokens and the log probability of the mask are returned.

\paragraph{Decoder.} 
The decoder itself is also an encoder-decoder model (the encoder of this model is referred to as encoder*).
The encoder* first embeds the tokens of the subsequence.
It then encodes the embedding using a bi-directional LSTM. 

The decoder decodes the full sentence. 
It therefore embeds the already decoded sequence (or just the <sos> symbol if there is none) into a 32-dimensional vector and concatenates the last hidden state of the encoder* to it. 
This embedding is the input for another uni-directional LSTM. 
Finally, the probability of the next word is calculated using global attention \cite{LuongAttention, bahdanau2016neuralmachinetranslationjointly} and the full sentence and its log probability are returned. 

During training, the decoder is given the keywords and at every step the model takes the correct word from the target sequence and calculates the probability of that sequence.
During evaluation, however, the decoder uses a greedy decoding strategy and thus takes the word with the highest probability as the next word. 
% TODO: <sos> anders verwoorden

\paragraph{Optimization.} 
The goal of the model is to be as efficient and accurate as possible. 
If equation \ref{eq:cost} and \ref{eq:loss} are merged and a parameter $\lambda$ is added to represent the trade-off between the two, the goal becomes the following:
\begin{equation}
    \label{eq:loss2}
    \min_{\alpha, \beta} \mathbb{E} [\text{cost}(x, \alpha)] + \lambda \cdot \mathbb{E}[\text{loss}(x, \alpha, \beta)].
\end{equation}
Here the expactation is taken over $x$. 
Since the gradients of equation \ref{eq:loss2} cannot be calculated, it is approximated with Monte Carlo.
The gradients can then by calculated as following (see appendix \ref{app:gradients} for the exact derivations):

\begin{equation}
    \label{eq:gradient_alpha}
    \nabla_\alpha F(x, z, \alpha, \beta) = \mathbb{E}_{q_{\alpha}(z|x)} [\nabla_{\alpha} \log q_{\alpha}(z|x) f(x, z, \beta)],
\end{equation}
\begin{equation}
    \label{eq:gradient_beta}
    \nabla_\beta F(x, z, \alpha, \beta) = \mathbb{E}_{q_{\alpha}(z|x)} [\nabla_{\beta} f (x, z, \beta)].
\end{equation}
Where, $f$ and the score function estimator (SFE) $F$ (and its Monte Carlo approximation) are:
\begin{equation}
    \label{eq:f}
    f(x, z, \beta) = \text{length}(z) + \lambda \cdot (-\log p_{\beta}(x|z)),
\end{equation}
\begin{equation}
    \label{eq:F}
    F (\alpha, \beta) = \mathbb{E}_{q_{\alpha}(z|x)} [f(x, z, \beta)],
\end{equation}
\begin{equation}
    \label{eq:F_approx}
    F (\alpha, \beta) \approxtext{M.C.} \frac{1}{M} \sum_i f (x, z^{(i)}, \beta).
\end{equation}
Here $M$ is the amount of samples drawn from $q_{\alpha}(z|x)$.

\subsection{Hyperparameters}
\label{sec:hyper}
During training, the sentences start with a <sos> symbol and end with a <eos> symbol.
During evaluation of the model, those symbols are removed.

The model is implemented using PyTorch \cite{pytorch}.
After each token is embedded, ReLu is used. During training, dropout is used to prevent overfitting. 
The hidden dimensions of the LSTM layers are all 32. 
To optimize the model an Adam optimizer \cite{adam} is used with a learning rate of 0.01. 
Multiple models were trained for 10 epochs with different values for $\lambda$.

\subsection{Optimizations}
\label{sec:optimizations}
To let the model predict better results faster, a few optimizations were done.

\paragraph{Copy mechanism.} The copy mechanism \cite{copyMechanism} in the decoder determines if it wants to copy the current token or wants to generate a new token. 
It therefore calculates the probability of the new word as following:
\begin{equation}
    p(w) = (1 - p_{\text{gen}}) \cdot p_{\text{copy}}(w) + p_{\text{gen}} \cdot p_{\text{word}}(w).
\end{equation}
For the calculation of the probability of the to be copied word, $p_{\text{copy}}(w)$, the global attention mechanism from the decoder is used. 
To calculate the probability of generating a new word, $p_{\text{gen}}$, the last hidden decoder state, the attention mechanism and the token embedding of the new to generated word are used in a linear layer followed by a sigmoid.
$p_{\text{word}}(w)$ is the probability of the to be generated word if a word is being generated. 

\paragraph{Adjusted vocabulary.} The model makes a vocabulary to translate words to numbers. 
During the evaluation of the model, a lot of unknown tokens were encountered.
This was due to words that were not present in the training set but were in the validation set. 
To solve this problem, the vocobulary was made using the whole dataset, instead of just the training set. 
\citeA{autocomplete} used the copy mechanism and a dynamic vocabulary for this, but for the task at hand and the amount of data this also works.

\paragraph{Variance reduction.} Because of the use of a SFE the gradient can be more prone to noise \cite{niculae2023discretelatentstructureneural}.
To solve this problem, its variance can be reduced in a couple of ways. 
One of those solutions is to sample a second subsentence in the encoder \cite{RennieStevenJ2017SSTf}. 
With this second subsentence, $f(z')$ can be determined and the loss can be updated as following:
\begin{equation}
    \mathbb{E}_{q_{\alpha}(z|x)} [\nabla_w \log q_{\alpha}(z|x) \cdot (f(x, z, \beta)-f(x, z', \beta))] + \mathbb{E}_{q_{\alpha}(z|x)} [\nabla_w f(x, z, \beta)],
\end{equation}
where $w$ is a parameter (i.e. either $\alpha$ or $\beta$). Note that the gradient is not changed when subtracting $f(z')$ from $f(z)$ since it can be seen as constant because it is independent of $z$.

\section{Results}
