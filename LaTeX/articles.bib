@article{autocomplete,
abstract = {We study textual autocomplete---the task of predicting a full sentence from a
partial sentence---as a human-machine communication game. Specifically, we
consider three competing goals for effective communication: use as few tokens
as possible (efficiency), transmit sentences faithfully (accuracy), and be
learnable to humans (interpretability). We propose an unsupervised approach
which tackles all three desiderata by constraining the communication scheme to
keywords extracted from a source sentence for interpretability and optimizing
the efficiency-accuracy tradeoff. Our experiments show that this approach
results in an autocomplete system that is 52% more accurate at a given
efficiency level compared to baselines, is robust to user variations, and saves
time by nearly 50% compared to typing full sentences.},
copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
keywords = {Computer Science - Computation and Language ; Computer Science - Learning},
language = {eng},
title = {Learning Autocomplete Systems as a Communication Game},
year = {2019},
author = {Lee, Mina and Hashimoto, Tatsunori B and Liang, Percy},
}

@inproceedings{Bar-YossefZiv2011Cqa,
abstract = {Query auto completion is known to provide poor predictions of the user's query when her input prefix is very short (e.g., one or two characters). In this paper we show that context, such as the user's recent queries, can be used to improve the prediction quality considerably even for such short prefixes. We propose a context-sensitive query auto completion algorithm, NearestCompletion, which outputs the completions of the user's input that are most similar to the context queries. To measure similarity, we represent queries and contexts as high-dimensional term-weighted vectors and resort to cosine similarity. The mapping from queries to vectors is done through a new query expansion technique that we introduce, which expands a query by traversing the query recommendation tree rooted at the query.
In order to evaluate our approach, we performed extensive experimentation over the public AOL query log. We demonstrate that when the recent user's queries are relevant to the current query she is typing, then after typing a single character, NearestCompletion's MRR is 48% higher relative to the MRR of the standard MostPopularCompletion algorithm on average. When the context is irrelevant, however, NearestCompletion's MRR is essentially zero. To mitigate this problem, we propose HybridCompletion, which is a hybrid of NearestCompletion with MostPopularCompletion. HybridCompletion is shown to dominate both NearestCompletion and MostPopularCompletion, achieving a total improvement of 31.5% in MRR relative to MostPopularCompletion on average.},
booktitle = {Proceedings of the 20th international conference on world wide web},
isbn = {9781450306324},
keywords = {context-awareness ; query auto-completion ; query expansion},
language = {eng},
pages = {107-116},
publisher = {ACM},
title = {Context-sensitive query auto-completion},
year = {2011},
author = {Bar-Yossef, Ziv and Kraus, Naama},
}

@article{SvyatkovskiyAlexey2019PACC,
abstract = {In this paper, we propose a novel end-to-end approach for AI-assisted code
completion called Pythia. It generates ranked lists of method and API
recommendations which can be used by software developers at edit time. The
system is currently deployed as part of Intellicode extension in Visual Studio
Code IDE. Pythia exploits state-of-the-art large-scale deep learning models
trained on code contexts extracted from abstract syntax trees. It is designed
to work at a high throughput predicting the best matching code completions on
the order of 100 $ms$.
We describe the architecture of the system, perform comparisons to
frequency-based approach and invocation-based Markov Chain language model, and
discuss challenges serving Pythia models on lightweight client devices.
The offline evaluation results obtained on 2700 Python open source software
GitHub repositories show a top-5 accuracy of 92\%, surpassing the baseline
models by 20\% averaged over classes, for both intra and cross-project
settings.},
copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
keywords = {Computer Science - Learning ; Computer Science - Software Engineering},
language = {eng},
title = {Pythia: AI-assisted Code Completion System},
year = {2019},
author = {Svyatkovskiy, Alexey and Zhao, Ying and Fu, Shengyu and Sundaresan, Neel},
}

@misc{data,
    author = {Yelp},
    title = {Yelp Dataset Challenge, Round 8},
    url = {https://www.yelp.com/dataset_challenge},
    year = {2017}
}

@article{GuuKelvin2018GSbE,
abstract = {We propose a new generative language model for sentences that first samples a prototype sentence from the training corpus and then edits it into a new sentence. Compared to traditional language models that generate from scratch either left-to-right or by first sampling a latent sentence vector, our prototype-then-edit model improves perplexity on language modeling and generates higher quality outputs according to human evaluation. Furthermore, the model gives rise to a latent edit vector that captures interpretable semantics such as sentence similarity and sentence-level analogies.},
author = {Guu, Kelvin and Hashimoto, Tatsunori B. and Oren, Yonatan and Liang, Percy},
address = {One Rogers Street, Cambridge, MA 02142-1209, USA},
copyright = {2018. This work is published under https://creativecommons.org/licenses/by/4.0/legalcode (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
keywords = {Approximation ; Computational linguistics ; Editing ; Food ; Language ; Language modeling ; Linguistics ; Machine translation ; Prototypes ; Semantics ; Sentences},
language = {eng},
pages = {437-450},
publisher = {MIT Press},
title = {Generating Sentences by Editing Prototypes},
volume = {6},
year = {2018},
}

@misc{niculae2023discretelatentstructureneural,
      title={Discrete Latent Structure in Neural Networks}, 
      author={Vlad Niculae and Caio F. Corro and Nikita Nangia and Tsvetomila Mihaylova and André F. T. Martins},
      year={2023},
      eprint={2301.07473},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2301.07473}, 
}

@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@article{RennieStevenJ2017SSTf,
abstract = {Recently it has been shown that policy-gradient methods for reinforcement
learning can be utilized to train deep end-to-end systems directly on
non-differentiable metrics for the task at hand. In this paper we consider the
problem of optimizing image captioning systems using reinforcement learning,
and show that by carefully optimizing our systems using the test metrics of the
MSCOCO task, significant gains in performance can be realized. Our systems are
built using a new optimization approach that we call self-critical sequence
training (SCST). SCST is a form of the popular REINFORCE algorithm that, rather
than estimating a "baseline" to normalize the rewards and reduce variance,
utilizes the output of its own test-time inference algorithm to normalize the
rewards it experiences. Using this approach, estimating the reward signal (as
actor-critic methods must do) and estimating normalization (as REINFORCE
algorithms typically do) is avoided, while at the same time harmonizing the
model with respect to its test-time inference procedure. Empirically we find
that directly optimizing the CIDEr metric with SCST and greedy decoding at
test-time is highly effective. Our results on the MSCOCO evaluation sever
establish a new state-of-the-art on the task, improving the best result in
terms of CIDEr from 104.9 to 114.7.},
author = {Rennie, Steven J and Marcheret, Etienne and Mroueh, Youssef and Ross, Jarret and Goel, Vaibhava},
copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
keywords = {Computer Science - Artificial Intelligence ; Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Learning},
language = {eng},
title = {Self-critical Sequence Training for Image Captioning},
year = {2017},
}

@article{copyMechanism,
  author       = {Jiatao Gu and
                  Zhengdong Lu and
                  Hang Li and
                  Victor O. K. Li},
  title        = {Incorporating Copying Mechanism in Sequence-to-Sequence Learning},
  journal      = {CoRR},
  volume       = {abs/1603.06393},
  year         = {2016},
  url          = {http://arxiv.org/abs/1603.06393},
  eprinttype    = {arXiv},
  eprint       = {1603.06393},
  timestamp    = {Mon, 13 Aug 2018 16:47:52 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/GuLLL16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{LuongAttention,
  author       = {Minh{-}Thang Luong and
                  Hieu Pham and
                  Christopher D. Manning},
  title        = {Effective Approaches to Attention-based Neural Machine Translation},
  journal      = {CoRR},
  volume       = {abs/1508.04025},
  year         = {2015},
  url          = {http://arxiv.org/abs/1508.04025},
  eprinttype    = {arXiv},
  eprint       = {1508.04025},
  timestamp    = {Mon, 13 Aug 2018 16:46:14 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/LuongPM15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{bahdanau2016neuralmachinetranslationjointly,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1409.0473}, 
}

@article{LSTM,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is
. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
address = {One Rogers Street, Cambridge, MA 02142-1209, USA},
copyright = {1997 INIST-CNRS},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithmics. Computability. Computer arithmetics ; Algorithms ; Applied sciences ; Artificial intelligence ; Computer science; control theory; systems ; Connectionism. Neural networks ; Exact sciences and technology ; Learning ; Memory ; Memory Short-Term ; Models Neurological ; Models Psychological ; Nerve Net - physiology ; Neural Networks (Computer) ; Theoretical computing ; Time Factors},
language = {eng},
number = {8},
pages = {1735-1780},
publisher = {MIT Press},
title = {Long Short-Term Memory},
volume = {9},
year = {1997},
}

@misc{adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
}

@book{FFBS,
abstract = {"This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package--PMTK (probabilistic modeling toolkit)--that is freely available online"--Back cover.},
author = {Murphy, Kevin P.},
address = {Cambridge, Massachusetts},
booktitle = {Machine learning : a probabilistic perspective},
isbn = {9780262018029},
keywords = {Apprentissage automatique},
language = {eng},
lccn = {2012004558},
publisher = {MIT Press},
series = {Adaptive computation and machine learning series},
title = {Machine learning : a probabilistic perspective },
year = {2012},
}

@article{Viterbi,
abstract = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described.< >},
author = {Rabiner, L.R.},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
keywords = {Hidden Markov models ; Speech recognition ; Tutorial},
language = {eng},
number = {2},
pages = {257-286},
publisher = {IEEE},
title = {A tutorial on hidden Markov models and selected applications in speech recognition},
volume = {77},
year = {1989},
}

@article{hsmms,
author = {Kevin Murphy},
year = {2002},
pages = {1-13},
title = {Hidden semi-Markov models (HSMMs)}
}

@inproceedings{conditionalRandomFields,
 author = {Sarawagi, Sunita and Cohen, William W},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {L. Saul and Y. Weiss and L. Bottou},
 pages = {},
 publisher = {MIT Press},
 title = {Semi-Markov Conditional Random Fields for Information Extraction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2004/file/eb06b9db06012a7a4179b8f3cb5384d3-Paper.pdf},
 volume = {17},
 year = {2004}
}

@misc{kong2016segmentalrecurrentneuralnetworks,
      title={Segmental Recurrent Neural Networks}, 
      author={Lingpeng Kong and Chris Dyer and Noah A. Smith},
      year={2016},
      eprint={1511.06018},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1511.06018}, 
}