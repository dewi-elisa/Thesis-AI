@article{autocomplete,
abstract = {We study textual autocomplete---the task of predicting a full sentence from a
partial sentence---as a human-machine communication game. Specifically, we
consider three competing goals for effective communication: use as few tokens
as possible (efficiency), transmit sentences faithfully (accuracy), and be
learnable to humans (interpretability). We propose an unsupervised approach
which tackles all three desiderata by constraining the communication scheme to
keywords extracted from a source sentence for interpretability and optimizing
the efficiency-accuracy tradeoff. Our experiments show that this approach
results in an autocomplete system that is 52% more accurate at a given
efficiency level compared to baselines, is robust to user variations, and saves
time by nearly 50% compared to typing full sentences.},
copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
keywords = {Computer Science - Computation and Language ; Computer Science - Learning},
language = {eng},
title = {Learning Autocomplete Systems as a Communication Game},
year = {2019},
author = {Lee, Mina and Hashimoto, Tatsunori B and Liang, Percy},
}

@inproceedings{Bar-YossefZiv2011Cqa,
abstract = {Query auto completion is known to provide poor predictions of the user's query when her input prefix is very short (e.g., one or two characters). In this paper we show that context, such as the user's recent queries, can be used to improve the prediction quality considerably even for such short prefixes. We propose a context-sensitive query auto completion algorithm, NearestCompletion, which outputs the completions of the user's input that are most similar to the context queries. To measure similarity, we represent queries and contexts as high-dimensional term-weighted vectors and resort to cosine similarity. The mapping from queries to vectors is done through a new query expansion technique that we introduce, which expands a query by traversing the query recommendation tree rooted at the query.
In order to evaluate our approach, we performed extensive experimentation over the public AOL query log. We demonstrate that when the recent user's queries are relevant to the current query she is typing, then after typing a single character, NearestCompletion's MRR is 48% higher relative to the MRR of the standard MostPopularCompletion algorithm on average. When the context is irrelevant, however, NearestCompletion's MRR is essentially zero. To mitigate this problem, we propose HybridCompletion, which is a hybrid of NearestCompletion with MostPopularCompletion. HybridCompletion is shown to dominate both NearestCompletion and MostPopularCompletion, achieving a total improvement of 31.5% in MRR relative to MostPopularCompletion on average.},
booktitle = {Proceedings of the 20th international conference on world wide web},
isbn = {9781450306324},
keywords = {context-awareness ; query auto-completion ; query expansion},
language = {eng},
pages = {107-116},
publisher = {ACM},
title = {Context-sensitive query auto-completion},
year = {2011},
author = {Bar-Yossef, Ziv and Kraus, Naama},
}

@article{SvyatkovskiyAlexey2019PACC,
abstract = {In this paper, we propose a novel end-to-end approach for AI-assisted code
completion called Pythia. It generates ranked lists of method and API
recommendations which can be used by software developers at edit time. The
system is currently deployed as part of Intellicode extension in Visual Studio
Code IDE. Pythia exploits state-of-the-art large-scale deep learning models
trained on code contexts extracted from abstract syntax trees. It is designed
to work at a high throughput predicting the best matching code completions on
the order of 100 $ms$.
We describe the architecture of the system, perform comparisons to
frequency-based approach and invocation-based Markov Chain language model, and
discuss challenges serving Pythia models on lightweight client devices.
The offline evaluation results obtained on 2700 Python open source software
GitHub repositories show a top-5 accuracy of 92\%, surpassing the baseline
models by 20\% averaged over classes, for both intra and cross-project
settings.},
copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
keywords = {Computer Science - Learning ; Computer Science - Software Engineering},
language = {eng},
title = {Pythia: AI-assisted Code Completion System},
year = {2019},
author = {Svyatkovskiy, Alexey and Zhao, Ying and Fu, Shengyu and Sundaresan, Neel},
}

@misc{data,
    author = {Yelp},
    title = {Yelp Dataset Challenge, Round 8},
    url = {https://www.yelp.com/dataset_challenge},
    year = {2017}
}

@article{GuuKelvin2018GSbE,
abstract = {We propose a new generative language model for sentences that first samples a prototype sentence from the training corpus and then edits it into a new sentence. Compared to traditional language models that generate from scratch either left-to-right or by first sampling a latent sentence vector, our prototype-then-edit model improves perplexity on language modeling and generates higher quality outputs according to human evaluation. Furthermore, the model gives rise to a latent edit vector that captures interpretable semantics such as sentence similarity and sentence-level analogies.},
author = {Guu, Kelvin and Hashimoto, Tatsunori B. and Oren, Yonatan and Liang, Percy},
address = {One Rogers Street, Cambridge, MA 02142-1209, USA},
copyright = {2018. This work is published under https://creativecommons.org/licenses/by/4.0/legalcode (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
keywords = {Approximation ; Computational linguistics ; Editing ; Food ; Language ; Language modeling ; Linguistics ; Machine translation ; Prototypes ; Semantics ; Sentences},
language = {eng},
pages = {437-450},
publisher = {MIT Press},
title = {Generating Sentences by Editing Prototypes},
volume = {6},
year = {2018},
}

@misc{niculae2023discretelatentstructureneural,
      title={Discrete Latent Structure in Neural Networks}, 
      author={Vlad Niculae and Caio F. Corro and Nikita Nangia and Tsvetomila Mihaylova and André F. T. Martins},
      year={2023},
      eprint={2301.07473},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2301.07473}, 
}

@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}