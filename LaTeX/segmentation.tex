In the second experiment, the model from chapter \ref{ch:repl} is expanded with a segmentation model. 

\section{Method}
An autoencoder model is made in which the encoder uses a segmentation model to choose the sequence of keywords $z$.
The same data was used as described in section \ref{sec:data}.

\subsection{Score matrix}
In order for this model to work, it needs a score tensor, $\boldsymbol{A}$.
This tensor consists of two upper triangle matrices of $(m+1) \times (m+1)$, called $A_0$ and $A_1$.
Both matrices represent the score of segments in the DAG made for each sentence as described in \ref{sec:segmentation}. 
The matrix $A_1$ represents the scores for when the segment is true, and $A_0$ for when the segment is false.
The matrix $A_0$ consists of only zeros to simplify the model.

To construct $A_1$, an embedding matrix $H$ with dimensions $m \times d$ is used.
Here $m$ is the amount of tokens in $x$ and $d$ the hidden dimension of the LSTM.
In addition, a weight matrix $W$ with dimensions $d \times d$ is used.
This weight matrix is learned by the encoder. 
Matrix $A_1$ is then calculated as following:
\begin{equation}
    A_{ij1} = h_i^T W h_j
\end{equation}
And more efficiently, with matrices:
\begin{equation}
    A_1 = H W H^T
\end{equation}
Note that the scores learned for $A_1$ can also be negative. 
Therefore, the model can prefer to leave out a segment when needed.

\subsection{Model description}
% TODO: logsumexp verwoorden
The same model as in section \ref{sec:model} was used. 
Only the encoder part was adjusted.
Instead of using an LSTM and a linear layer to score each token, the segmentation model is used.

\paragraph{Encoder.} The encoder embeds each token. 
It then uses a one-directional LSTM. 
The output of this LSTM is the matrix $H$, which is used to make the score matrix $a$, as described in the previous section. 
Then, using dynamic programming algorithms, a segmentation is sampled and its probability is calculated. 
As described in section \ref{sec:optimizations}, variance reduction is applied and thus also a second segmentation is sampled.
These segmentations are then converted into masks.

\paragraph{Dynamic programming.} To sample a segmentation, the forward filtering, backward sampling algorithm \shortcite{FFBS} was used.
This algorithm first calculates the logsumexp of all possible segmentations and then samples from this distribution.
Pseudocode for this algorithm can be found in algorithm \ref{alg:forward}.

To calculate the probability of the segmentation, the forward algorithm was used.
This algorithm calculates the logsumexp of all possible segmentations.
Pseudocode for this algorithm can be found in algorithm \ref{alg:ffbs}.
The logsumexp is then used in combination with the score of the segmentation (which can be calculated using a) to calculate the probability of the segmentation

\begin{algorithm}
\caption{Forward filtering, backward sampling algorithm}
\label{alg:ffbs}
\begin{algorithmic}
\Require score tensor $A$ with shape $(m+1) \times (m+1) \times 2$
\Ensure sampled segmentation
\State
\State $n \gets A.\text{shape[0]}$
\State $q_0 \gets 0$
\For{$i=1, \dots, n$}
    \State $q_i \gets \log \sum_{0 \leq j < i} (\exp (q_j + a_{ji0}) + \exp (q_j + a_{ji1}))$
\EndFor
\State
\State $y \gets []$
\State $i \gets n - 1$
\While{$i > 0$}
    \State sample $j < i$ and $k$ with probability $p_j \gets \exp (a_{ji0} + q_j - q_i) + \exp(a_{ji1} + q_j - q_i)$
    \State $y \gets (ji,k) \frown y$
    \State $i \gets j$
\EndWhile
\State
\State \Return $y$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Forward algorithm}
\label{alg:forward}
\begin{algorithmic}
\Require score tensor $A$ with shape $(m+1) \times (m+1) \times 2$
\Ensure log-normalizer
\State
\State $n \gets A.\text{shape[0]}$
\State $q_0 \gets 0$
\For{$i=1, \dots, n$}
    \State $q_i \gets \log \sum_{0 \leq j < i} (\exp (q_j + a_{ji0}) + \exp (q_j + a_{ji1}))$
\EndFor
\State
\State \Return $q_n$
\end{algorithmic}
\end{algorithm}

% \subsection{Hyperparameters}
% The same hyperparameters as in section \ref{sec:hyper} were used.
% In addition, for the segmentation model ...
% TODO: describe parameters of the segmentation model

\section{Results}
