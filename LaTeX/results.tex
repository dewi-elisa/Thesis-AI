% TODO: legenda weg (acc cost)
% TODO: validation oranje maken + vierkantjes ipv rondjes (acc cost)
% TODO: figuren in elkaar zetten (acc cost)
% TODO: verwijzen naar figuren
% TODO: legenda ergens ander heen (cost) 
Six models, with different values for $\lambda$, were trained for each model type.
To evaluate the models, the accuracy and the cost were calculated.
This was done for the last epoch, and on both the training and validation set.
All figures are in appendix \ref{app:results}.

\paragraph{Accuracy.}
Figures with the accuracy and cost are shown in figure \ref{fig:acc_cost}.
Accuracy is measured on the whole sentence. This means that even when only one word in $y$ is different from $x$ the sentence will have an accuracy of 0.
For each model, the average is taken over the whole dataset and only the accuracy from the last epoch is reported.
In both cases, we can see a higher accuracy on the validation set (between 98\% and 100\%) than on the training set (around 59.5\%). 

\paragraph{Cost.}
Cost is measured as the percentage of words kept in the keywords:
\begin{equation}
    \label{eq:cost_perc}
    \text{cost} = \frac{\text{number of words in } z}{\text{number of words in } x} \cdot 100.
\end{equation}
How the cost progresses per epoch can be seen in figure \ref{fig:cost}.
The percentage of words kept as keywords is fairly constant for the segmentation model (Â±91\%) in comparison to the unstructured model (between 82\% and 100\%). 

For the unstructured model we see that the lower the $\lambda$ the higher the tendency for the model to drop words. 
However, when $\lambda$ is bigger than 1.5, the model seems to be wanting to keep all tokens.
For the segmentation model however, all models keep 91\% of the tokens, no matter what value of $\lambda$ they are trained on.

\paragraph{Parameter $\boldsymbol{\lambda}$.}
For the unstructured model, the higher the $\lambda$ the more words are kept as keywords in general.
To see if the model can drop more words (and thus get a lower cost) with a higher $\lambda$, another model was trained on only 10 training samples. 
The results of this model can be seen in figure \ref{fig:high}.
It can be seen that the model can learn to drop words, however, to reduce the loss it increases the cost fast.
It is interesting, however, that in this case the training accuracy is higher than the validation accuracy.

\paragraph{Training objective.}
The training objective goes down for both models (see figure \ref{fig:obj}).
In general, the higher the value of $\lambda$, the higher the training objective after 10 epochs for the unstructured model.
For the segmentation model, all values of $\lambda$ seem to converge to the same value for the training objective.

\paragraph{Text analysis.}
The models printed a few examples every epoch. 
An example consists of the sentence $x$, the keywords $z$ and the predicted output sentence $y$.
When looking at the examples for both models, we see that occasionally, the model drops the <sos> symbol but nothing more than that. 
An example can be seen in table \ref{tab:examples}.

\begin{table}
    \centering
    \begin{tabular}{|ll|}
        \hline
        $x$ & <sos> the shrimp burrito was awesome . <eos>  \\
        $z$ & the shrimp burrito was awesome . <eos> \\
        $y$ & <sos> the shrimp burrito was awesome . <eos> \\
        \hline
    \end{tabular}
    \caption{Example sentence}
    \label{tab:examples}
\end{table}
