\documentclass[a4paper, 12pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{xcolor}
\usepackage{afterpage}

\usepackage{relsize}
\usepackage{moresize}

\usepackage{graphicx}
\usepackage{geometry}

\usepackage{hyperref}
\usepackage{apacite}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{caption}
\usepackage{subcaption}

\newcommand{\approxtext}[1]{\ensuremath{\stackrel{\text{#1}}{\approx}}}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

% [CHANGE] The title of your thesis. If your thesis has a subtitle, then this
% should appear right below the main title, in a smaller font.
\newcommand{\theTitle}{The first sentence \\
\vspace{0.5em}
the second sentence}
\newcommand{\theSubTitle}{a smaller subtitle}


% [CHANGE] Your full name. In case of multiple names, you can include their
% initials as well, e.g. "Robin G.J. van Achteren".
\newcommand{\theAuthor}{Dewi E. Timman}

% [CHANGE] Your student ID, as this has been assigned to you by the UvA
% administration.
\newcommand{\theStudentID}{12419273}

% [CHANGE] The name of your supervisor(s). Include the titles of your supervisor(s),
% as well as the initials for *all* of his/her first names.
\newcommand{\theSupervisor}{Dr. V. Niculae} % Dr. Ing. L. Dorst

% [CHANGE] The address of the institute at which your supervisor is working.
% Be sure to include (1) institute (is appropriate), (2) faculty (if
% appropriate), (3) organisation name, (4) organisation address (2 lines).
\newcommand{\theInstitute}{
Informatics Institute \\ %Institute for Logic, Language and Computation
Faculty of Science\\
University of Amsterdam\\
Science Park 900 \\ 
1098 XH Amsterdam 
}

% [CHANGE] The semester in which you started your thesis.
\newcommand{\theDate}{Semester 1, 2023-2024}

\begin{document}
\pagestyle{empty}
\begin{center}

\vspace{2.5cm}


\begin{Huge}
% see definition at beginning of document
\theTitle
\end{Huge} \\

\vspace{0.5 cm}

\begin{Large}
\theSubTitle
\end{Large}

\vspace{1.5cm}

% see definition at beginning of document
\theAuthor\\
% see definition at beginning of document
\theStudentID

\vspace{1.5cm}

% [DO NOT CHANGE]
Bachelor thesis\\
Credits: 18 EC

\vspace{0.5cm}

% [DO NOT CHANGE] The name of the educational programme.
Bachelor \textit{Kunstmatige Intelligentie} \\
\vspace{0.25cm}
\includegraphics[width=0.075\paperwidth]{figs/uva_logo} \\
\vspace{0.1cm}

% [DO NOT CHANGE] The address of the educational programme.
University of Amsterdam\\
Faculty of Science\\
Science Park 900\\
1098 XH Amsterdam

\vspace{2cm}

\emph{Supervisor}\\

% see definition at beginning of document
\theSupervisor

\vspace{0.25cm}

% see definition at beginning of document
\theInstitute

\vspace{1.0cm}

% see definition at beginning of document
\theDate

\end{center}
\newpage

\pagenumbering{arabic}
\setcounter{page}{1}
\pagestyle{plain} 

\chapter*{Abstract}
% TODO
\textbf{keywords:} 

\tableofcontents

\chapter{Introduction}
% What am I researching? bruggetje met taal
% \begin{itemize}
%     \item keywords $\rightarrow$ sentence
%     \item efficient, accurate, interpretable
%     \item this research: segmentation model more effective, natural, flexible than a bit mask?
% \end{itemize}

What if machines can read our mind? 
If we can give a machine a few keywords and let the machine generate a sentence from these keywords, our lives would become more productive and efficient. 
This is what autocomplete systems are trying to achieve. 
\textcolor{orange}{The way in which we choose the keywords is also important. 
Taking just the first or the last few words of a sentence as keywords usually does not capture the full meaning of the sentence.} 
For example, if someone want to capture the meaning of \textit{'I live in Amsterdam'} in a few keywords, the words \textit{'live Amsterdam'} would probably be choosen. 
Thus, the keywords come from multiple places in the sentence. 
Therefore, autocomplete systems need to use more complex models to be more efficient and accurate. 

\section{Literature review}
% TODO/delete

\subsection{Autocomplete communication game}

\begin{figure}
    \centering
    \includegraphics[width=.5\linewidth]{figs/autocomplete_game.png}
    \caption{schematic overview of the communication game. Figure from \protect\citeA{autocomplete}.}
    \label{fig:autocomplete}
\end{figure}

The same autocomplete communication game is considered as in \citeA{autocomplete}. In this game, a human (called user) encodes a sentence into keywords. 
These keywords are then decoded by a machine (called system) to retrieve the full, initial sentence. 
A schematic overview is given in figure \ref{fig:autocomplete}. 
The communication game is succesfull if the retrieved sentence is the same as the initial sentence. 

More formally, a target sentence $x=(x_1, \dots, x_m)$ is communicated by a user through the keywords $z=(z_1, \dots, z_n)$. 
Note that $z$ is a subsequence of $x$. 
The system then tries to retrieve the target sentence by decoding the keywords. 
The target sentence is described by the keywords using encoding strategy $q_{\alpha}(z | x)$ and the system decodes the keywords by using decoding strategy $p_{\beta}(x|z)$. 

For a model to be efficient, the number of keywords needs to be as low as possible. 
In addition, for a model to be accurate, the probability of reconstructing $x$ from $z$ needs to be as high as possible. 
Therefore, a cost and a loss, respectively, can be defined:
\begin{equation}
    \label{eq:cost}
    \text{cost}(x,\alpha) = \mathbb{E}_{q_{\alpha}(z|x)} [\text{length}(z)]
\end{equation}
\begin{equation}
    \label{eq:loss}
    \text{loss}(x,\alpha,\beta) = \mathbb{E}_{q_{\alpha}(z|x)} [-\log p_{\beta}(x|z)]
\end{equation}

\subsection{Segmentation model}
% Why does a segmentation model work?
% TODO: better captions for images
% TODO: add references
% TODO: add Markov Models

\subsubsection{General idea}
\textcolor{orange}{With a segmentation model all possible segmentation can be made. 
A segmentation model scores every possible segmentation.
With these scores, the model can determine what the best possible segmentation is.}

\subsubsection{Segmentation model for text}
So how does the segmentation model work for text? 
If we have a sentence, e.g. \textit{'I will be late'}, we can use fencepost indexing and represent the fenceposts as nodes in a directed acyclic graph (DAG). 
We can then draw edges between those nodes that represent segments. 
Those segments can be seen as (groups of) words. 
In figure \ref{fig:dag}, a DAG can be seen in which all the possible segments are showed. 
In the case of the autocomplete communication model described before, a segment is either kept or not.
Therefore, we can have one edge representing 'keep' and one representing 'do not keep', resulting in figure \ref{fig:dag2}.
If the pink edges are taken as 'do not keep' and the blue ones as 'keep', two possible segmentations can be seen in figure \ref{fig:segmentation} and \ref{fig:segmentation2}. 
Both segmentations result in the keywords \textit{'will be late'}. 

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/segmentation1.jpeg}
        \caption{Segments as a DAG}
        \label{fig:dag}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/segmentation2.jpeg}
        \caption{More segments as a DAG}
        \label{fig:dag2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/segmentation3.jpeg}
        \caption{Possible segmentation}
        \label{fig:segmentation}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/segmentation4.jpeg}
        \caption{Another possible segmentation}
        \label{fig:segmentation2}
    \end{subfigure}
    \caption{Segmentation model.}
    \label{fig:segmentation_model}
\end{figure}

All segments can be scored with the help of the Forward algorithm. 
After scoring each segment, the model can then choose the segments with the highest scores to retrieve the best possible segmentation. 

\subsection{Structured latent variables}
How does the model work? 

\section{Current research}
% Gap
% Research question: To what extend can a segmentation model help by selecting keywords for an auto-complete communication game?
% TODO: check references - are they relevant?

Previous research did not take structure into account \cite{autocomplete, Bar-YossefZiv2011Cqa, SvyatkovskiyAlexey2019PACC}.
Since language is structured, it makes sense to use a structured model as an autocomplete model. 
In this research, we look at how we can use a latent segmentation model to retrieve keywords from a sentence. 
The segmentation model will be implemented in the encoder of the encoder-decoder model in order to choose the best keywords from the sentence.

\chapter{Experiment 1 \\ Replication}
In the first experiment the autoencoder model from \citeA{autocomplete} was replicated. 

\section{Method}
% TODO: write something here

\subsection{Data}
% What data? Data preprocessing? Data split?
% TODO: They also say something about capitalisation -> do we do this too?
% TODO: make a table with a few example sentences
% TODO: descirbe train, validation and test set
The same data was used as in \citeA{autocomplete}.
The data used to train the model consisted of 500K randomly sampled sentences from the Yelp restaurant reviews corpus \cite{data}.
Another 10K sentences were used to test the model. 
The sentences had at most 16 tokens. 
The reviews were segmented into sentences following the same procedure as in \citeA{GuuKelvin2018GSbE}. 

\subsection{Experimental Design}
% What is the setup?
% TODO: check if it is indeed y_k, maybe y_m?
The model used is an encoder-decoder model.
The encoder, using the encoding strategy $q_{\alpha}(z|x)$, takes as input a target sequence $x = (x_1, \dots, x_m)$ and outputs a sequence of keywords $z = (z_1, \dots, z_n)$.
The decoder, using the decoding strategy $p_{\beta}(x|z)$, then takes these keywords as input and outputs a predicted sequence $y = (y_1, \dots, y_k)$. 
The better the autoencoder works, the more likely it is that $x$ and $y$ are equal. 

\subsection{Model description}
% What does the model look like? Which equations are used? Diagrams/pseudocode?
\paragraph{Encoder.} 
The encoder embeds the tokens and uses a uni-directional LSTM to score the tokens.
An additional linear layer followed by sigmoid function is used to determine the probability of keeping each token. 
From these probabilities, a mask is sampled from a Bernoulli distribution. 
Finally, the sequence of kept tokens and the log probability of the mask are returned.

\paragraph{Decoder.} 
The decoder itself is also an encoder-decoder model (the encoder of this model is referred to as encoder*).
The encoder* first embeds the tokens of the subsequence.
It then encodes the embedding using a bi-directional LSTM. 

The decoder decodes the full sentence. 
It therefore embeds the already decoded sequence (or just the <sos> symbol if there is none) into a 300-dimensional vector and concatenates the last hidden state of the encoder* to it. 
This embedding is the input for another uni-directional LSTM. 
Finally, the probability of the next word is calculated using global attention and the full sentence and its log probability are returned. 
% teacher forcing tijdens training, greedy tijdens test
% <sos> anders verwoorden

\paragraph{Optimization.} 
The goal of the model is to be as efficient and accurate as possible. 
If equation \ref{eq:cost} and \ref{eq:loss} are merged and a parameter $\lambda$ is added to represent the trade-off between the two, the goal becomes the following:
\begin{equation}
    \label{eq:loss2}
    \min_{\alpha, \beta} \mathbb{E} [\text{cost}(x, \alpha)] + \lambda \cdot \mathbb{E}[\text{loss}(x, \alpha, \beta)].
\end{equation}
Here the expactation is taken over $x$. 
Since the gradients of equation \ref{eq:loss2} cannot be calculated, it is approximated with Monte Carlo.
The gradients can then by calculated as following (see appendix \ref{app:gradients} for the exact derivations):
% TODO: berekening in appendix
\begin{equation}
    \label{eq:gradient_alpha}
    \nabla_\alpha F(\alpha, \beta) = \mathbb{E}_{q_{\alpha}(z|x)} [\nabla_{\alpha} \log q_{\alpha}(z|x) f(z, \beta)],
\end{equation}
\begin{equation}
    \label{eq:gradient_beta}
    \nabla_\beta F(\alpha, \beta) = \mathbb{E}_{q_{\alpha}(z|x)} [\nabla_{\beta} f (z, \beta)].
\end{equation}
Where, $f$ and the score function estimator (SFE) $F$ (and its Monte Carlo approximation) are:
\begin{equation}
    \label{eq:f}
    f(z, \beta) = \text{length}(z) + \lambda \cdot (-\log p_{\beta}(x|z)),
\end{equation}
\begin{equation}
    \label{eq:F}
    F (\alpha, \beta) = \mathbb{E}_{q_{\alpha}(z|x)} [f(z, \beta)],
\end{equation}
\begin{equation}
    \label{eq:F_approx}
    F (\alpha, \beta) \approxtext{M.C.} \frac{1}{M} \sum_i f (z^{(i)}, \beta).
\end{equation}
Here $M$ is the amount of samples drawn from $q_{\alpha}(z|x)$ and $x$ is dropped for clarity.

\subsubsection{Hyperparameters}
During training, the sentences start with a <sos> symbol and end with a <eos> symbol.
During evaluation of the model, those symbols are removed.

The model is implemented using PyTorch \cite{pytorch}.
After each token is embedded, ReLu is used. During training, dropout is used to prevent overfitting. 
The hidden dimensions of the LSTM layers are all 300. 
To optimize the model an Adam optimizer is used with a learning rate of 0.01. 

\subsubsection{Optimizations}
To let the model predict better results faster, a few optimizations were done.
\paragraph{Copy mechanism.} The copy mechanism in the decoder determines if it wants to copy the current token or wants to generate a new token. 
It therefore calculates the probability of the new word as following:
\begin{equation}
    p(w) = (1 - p_{\text{gen}}) \cdot p_{\text{copy}}(w) + p_{\text{gen}} \cdot p_{\text{word}}(\text{w}).
\end{equation}
For the calculation of the probability of the to be copied word, $p_{\text{copy}}$, the global attention mechanism from the decoder is used. 
To calculate the probability of generating a new word, $p_{\text{gen}}$, the last hidden decoder state, the attention mechanism and the token embedding of the new to generated word are used in a linear layer followed by a sigmoid. 

\paragraph{Adjusted data.} The model of \citeA{autocomplete} also predicts capital letters and whitespaces. 
However, for English we can just assume that after every word a space is used. 
Therefore, the data was adjust to leave out the characters for whitespaces. 
Something similar is also true for capital letters: usually these only occur at the start of a sentence or in names. 
It is thus not necessary to also predict capital letters. 

In addition, the train and validation set were adjusted so that the validation set is a subset of the training set. 
A reason for this was the many unkown tokens in the validation set if this was not done. 
\citeA{autocomplete} used the copy mechanism and a dynamic vocabulary for this, but for the task at hand and the amount of data this also works.

\paragraph{Variance reduction.} Because of the use of a SFE the gradient can be more prone to noise \cite{niculae2023discretelatentstructureneural}.
To solve this problem, its variance can be reduced in a couple of ways. 
One of those solutions is to sample a second subsentence in the encoder. 
With this second subsentence, $f'$ can be determined and the loss can be calculated as following:
\begin{equation}
    \mathbb{E}_{q_w(Z)} [\nabla_w \log q_w(Z) \cdot (f(Z)-f')] + \mathbb{E}_{q_w(Z)} [\nabla_w f(Z)].
\end{equation}
Note that the gradient is not changed when subtracting $f'$ from $f(Z)$ since it is a constant which is independent of $w$.


\section{Results}

\chapter{Experiment 2 \\ Segmentation Model}
\section{Method}
\section{Results}

\chapter{Results}

\chapter{Conclusion}

\chapter{Discussion}

\bibliography{articles.bib}
\bibliographystyle{apacite}

\appendix
\chapter{Optimization derivations}
\label{app:gradients}
Using equations \ref{eq:cost} and \ref{eq:loss}, the goal is:
\begin{align*}
    \quad & \min_{\alpha, \beta} \mathbb{E} [\text{cost}(x, \alpha)] + \lambda \cdot \mathbb{E}[\text{loss}(x, \alpha, \beta)] \\
    &= \min_{\alpha, \beta} \mathbb{E} [\text{cost}(x, \alpha) + \lambda \cdot \text{loss}(x, \alpha, \beta)] \\
    &= \min_{\alpha, \beta} \frac{1}{D} \sum_{x \in D}[\text{cost}(x, \alpha) + \lambda \cdot \text{loss}(x, \alpha, \beta)].
\end{align*}

\noindent $f$ can then be defined as:
\begin{align*}
    f(z, \beta) &= \text{cost}(x, \alpha) + \lambda \cdot \text{loss}(x, \alpha, \beta) \\
    &= \text{length}(z) + \lambda \cdot (-\log p_{\beta}(x|z)).
\end{align*}

\noindent And $F$ as:
\begin{align*}
    F(\alpha, \beta) &= \mathbb{E}_{q_{\alpha}(z|x)} [[\text{length}(z) + \lambda \cdot (-\log p_{\beta}(x|z))]] \\
    &=^{\ast} \sum_{z\in Z} [q_{\alpha}(z|x) f(z, \beta)] \\
    &= \mathbb{E}_{q_{\alpha}(z|x)}[f(z, \beta)]
\end{align*}
where $Z$ consists of all the possible masks of size $x$. In the step marked with *, the following rule is used: $\mathbb{E}_{P(A)}[f(A)] = \sum_{a \in A}P(a)f(a)$. \\

\noindent Thus, the goal then becomes:
\begin{align*}
    \min_{\alpha, \beta} F(\alpha, \beta).
\end{align*}

\noindent Since
\begin{align*}
    F(\alpha, \beta) \approxtext{M.C.} \frac{1}{M} \sum_i f(z^{i}, \beta)
\end{align*}
we also have that:
\begin{align*}
    \nabla_{\alpha} F(\alpha, \beta) \approxtext{M.C.} \nabla_{\alpha} \sum_i f(z^{i}, \beta) \\
    \nabla_{\beta} F(\alpha, \beta) \approxtext{M.C.} \nabla_{\beta} \sum_i f(z^{i}, \beta).
\end{align*}
Where M.C. stands for Monte Carlo estimation. \\

\noindent Therefore, we can calculate the gradients with respect to $\alpha$ as following:
\begin{align*}
    \nabla_{\alpha} F(\alpha, \beta) &= \nabla_{\alpha} \mathbb{E}_{q_{\alpha}(z|x)} [f(z, \beta)] \\
    &= \nabla_{\alpha} \sum_z [q_{\alpha}(z|x) f(z, \beta)] \\
    &= \sum_z \nabla_{\alpha} (q_{\alpha}(z|x) f(z, \beta)) \\
    &=^\ast \sum_z (q_{\alpha}(z|x) [\nabla_{\alpha} \log q_{\alpha}(z|x) f(z, \beta)]) \\
    &= \mathbb{E}_{q_{\alpha}(z|x)} [\nabla_{\alpha} \log q_{\alpha}(z|x) f(z, \beta)].
\end{align*}
In the step marked with *, a log-derivative trick is used: $\nabla_t \log h(t) = \frac{\nabla_t h(t)}{h(t)}$. \\

\noindent And the gradient with respect to $\beta$:
\begin{align*}
    \nabla_{\beta} F(\alpha, \beta) &= \nabla_{\beta} \mathbb{E}_{q_{\alpha}(z|x)}[f(z, \beta)] \\
    &= \nabla_{\beta} \sum_z [q_{\alpha}(z|x)f(z, \beta)] \\
    &= \sum_z \nabla_{\beta} (q_{\alpha}(z|x)f(z, \beta)) \\
    &= \sum_z (q_{\alpha}(z|x) [\nabla_{\beta} f(z, \beta)]) \\
    &= \mathbb{E}_{q_{\alpha}(z|x)} [\nabla_{\beta} f(z, \beta)].
\end{align*}

\end{document}