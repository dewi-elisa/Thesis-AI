% repeat RQ + results
To see to what extent a segmentation model can help by selecting keywords in an autocomplete communication game, two different kinds of models were made and compared.
% summarize main findings
The results show no significant differences between the models.

% interpret findings
\paragraph{Accuracy.}
It is unusual for the accuracy to be higher on the validation set than on the training set. 
One possible reason for this can be an overlap between the two sets. 
However, after closer inspection this was not the case.
Other reasons can be the use of dropout or a small validation set. 

\paragraph{Cost.}
Both models have a high cost.
Therefore, the models find it hard to drop words. 
However, the unstructured model can have the tendency to keep all words in the sentence as keywords, especially when $\lambda$ is high.

\paragraph{Parameter $\boldsymbol{\lambda}$.}
For the segmentation model, the value of $\lambda$ does not seem to have any big difference on the accuracy, the cost and the training objective.
On the unstructured model, however, the higher the value of $\lambda$ the higher the cost, the accuracy and the training objective.
It is only logical for the accuracy to become higher when the cost increases, since more words are kept and therefore the model can just copy the keywords.
Consequently, the cost is part of the training objective and therefore the training objective will also be higher, especially when the value of $\lambda$ is high.
We can see that the model does drop more words with a higher $\lambda$, but it does not do so significantly. 
The training accuracy is higher than the validation accuracy, which can be a sign of overfitting. 
Figure \ref{fig:high_obj}, however, shows that the training objective does not share this phenomenon.


\paragraph{Training objective.}
The training objective does go down, but is still decreasing after 10 epochs for the unstructured model.
This can be a sign that the model still needs more training. 

\paragraph{Text analysis.}
When looking at the printed examples and comparing these to the cost, it seems that the lower the cost the more often the <sos> token is dropped. 